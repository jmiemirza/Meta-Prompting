<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs"/>
  <meta property="og:description" content=""/>
  <meta property="og:url" content="https://jmiemirza.github.io/Meta-Prompting/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Meta-Prompting</title>
  <link rel="icon" type="image/x-icon" href="static/images/car.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jmiemirza.github.io/" target="_blank">M. Jehanzeb Mirza</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://mitibmwatsonailab.mit.edu/people/leonid-karlinsky/" target="_blank">Leonid Karlinsky</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=JJRr8c8AAAAJ&hl=en" target="_blank">Wei Lin</a><sup>4</sup>,
                  </span>

                            <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=ER4dt8cAAAAJ&hl=ja" target="_blank">Sivan Doveh</a><sup>5,6</sup>
                  </span>

              </span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=hFcNhWEAAAAJ&hl=en" target="_blank">Jakub Micorek</a><sup>1</sup>
                  </span>

              </span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=oDDqnQ4AAAAJ&hl=en" target="_blank">Mateusz Kozinski</a><sup>1</sup>,

                </span>

              <span class="author-block">
                    <a href="https://hildekuehne.github.io/" target="_blank">Hilde Kuhene</a><sup>3,7</sup>
                  </span>

                  </span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=iWPrl3wAAAAJ&hl=en" target="_blank">Horst Possegger</a><sup>1</sup>
                  </span>
                  <br>
<!--              <span class="author-block">-->
<!--                <a href="https://www.rogerioferis.org/" target="_blank">Rogerio Feris</a><sup>2</sup>,</span>-->
<!--                <span class="author-block">-->
<!--              <span class="author-block">-->
<!--                    <a href="https://scholar.google.com/citations?user=_pq05Q4AAAAJ&hl=en" target="_blank">Horst Bischof</a><sup>1</sup>-->
<!--                  </span>-->
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>ICG, TU Graz, Austria. <sup>2</sup>CDL-EML. <sup>3</sup>MIT-IBM Watson AI Lab, USA. <sup>4</sup>JKU, Linz, Austria. <sup>5</sup>IBM Research, Israel.
                      <br><sup>6</sup>Weizmann Institute of Science, Israel. <sup>7</sup>University of Bonn, Germany.
                        <br>ArXiv Preprint</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jmiemirza/Meta-Prompting" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code & Data</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                <span class="link-block">-->
<!--                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>Abstract</span>-->
<!--                </a>-->
<!--              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/motivation.png" alt="Image Description" height="100%">
      <h2 class="subtitle has-text-justified is-size-6">
        Our MPVR utilizes a Meta Prompt, comprising a system prompt (instruction),
in-context example demonstrations (fixed throughout), and metadata (name and description) for a downstream task of interest. The Meta Prompt instructs an LLM to
generate diverse task-specific LLM queries, which are used to obtain category-specific
VLM prompts (visual text descriptions) by again querying the LLM after specifying
the class name. These category-specific VLM prompts are then ensembled into a
zero-shot classifier for recognizing the downstream task categories.
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs).
To obtain these category-specific prompts, the present methods rely on
hand-crafting the prompts to the LLMs for generating VLM prompts for
the downstream tasks. However, this requires manually composing these
task-specific prompts and still, they might not cover the diverse set of
visual concepts and task-specific styles associated with the categories of
interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose
Meta-Prompting for Visual Recognition (MPVR). Taking as input only
minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR
automatically produces a diverse set of category-specific prompts resulting in a strong zero-shot classifier. MPVR generalizes effectively across
various popular zero-shot image recognition benchmarks belonging to
widely different domains when tested with multiple LLMs and VLMs.
For example, MPVR obtains a zero-shot recognition improvement over
CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20
datasets) leveraging GPT and Mixtral LLMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
                  <h2 class="title is-3">Method and Results</h2>
    <div class="level-set has-text-justified">
                <p>
                    Given a pre-trained model and statistics of the clean activations from the training data, our ActMAD aligns the
activation responses from the shifted test data to the clean activations at test-time. We model the activation distributions in terms of the means
and variances of each activation, such that the statistics have the same shape as the feature maps. The statistics of the training activations are
pre-computed on the training set, or computed on unlabelled data without distribution shift.
                </p>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
         <img src="static/images/method.jpg" alt="MY ALT TEXT" style="width:85%;"/>

        <h2 class="subtitle has-text-centered">
          Schematic of ActMAD.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/teaser.jpg" alt="MY ALT TEXT"  style="width:85%;"/>
        <h2 class="subtitle has-text-centered">
          Continuous online adaptation for ActMAD in changing weather conditions.
        </h2> -->

        <section class="hero is-small">
          <div class="container is-max-desktop">
            <div class="hero-body">
                                  <h2 class="title is-3">Method</h2>
        
              <img src="static/images/main.png" alt="Image Description" height="20%">
              <h2 class="subtitle has-text-justified is-size-6">
                MPVR framework. In the first stage, a meta-prompt comprising of a system
prompt, in-context examples, and metadata consisting of downstream task specification
is queried to the LLM instructing it to generate multiple diverse task-specific LLM
queries, which are populated with the category of interest and again queried to the
LLM to obtain the category-level prompts for assembling a zero-shot classifier.
              </h2>
            </div>
          </div>
        </section>
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
                          <h2 class="title is-3">Main Results</h2>

      <img src="static/images/main_results.png" alt="Image Description" height="20%">
      <h2 class="subtitle has-text-justified is-size-6">
        Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/32 backbone from OpenAI CLIP. S-TEMP refer to the results obtained by using the default template (a photo of a class name), while DS-TEMP refer to the results obtained by using the ensemble of dataset specific prompts.
An empty placeholder for CUPL indicates that the respective baseline did not provide the handcrafted prompts for the dataset.
For Waffle, mean results from 7 random runs are reported, following the original publication.
      </h2>
    </div>
  </div>
</section>
<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
                  <h2 class="title is-3">Results</h2>
    <div class="level-set has-text-justified">
                <p>
                    Given a pre-trained model and statistics of the clean activations from the training data, our ActMAD aligns the
activation responses from the shifted test data to the clean activations at test-time. We model the activation distributions in terms of the means
and variances of each activation, such that the statistics have the same shape as the feature maps. The statistics of the training activations are
pre-computed on the training set, or computed on unlabelled data without distribution shift.
                </p>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
         <img src="static/images/method.jpg" alt="MY ALT TEXT" style="width:85%;"/>

        <h2 class="subtitle has-text-centered">
          Schematic of ActMAD.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/teaser.jpg" alt="MY ALT TEXT"  style="width:85%;"/>
        <h2 class="subtitle has-text-centered">
          Continuous online adaptation for ActMAD in changing weather conditions.
        </h2> -->

        <!--      </div>-->
<!--      <div class="item">-->
<!--        &lt;!&ndash; Your image here &ndash;&gt;-->
<!--        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>-->
<!--        <h2 class="subtitle has-text-centered">-->
<!--         Third image description.-->
<!--       </h2>-->
<!--     </div>-->
<!--     <div class="item">-->
<!--      &lt;!&ndash; Your image here &ndash;&gt;-->
<!--      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        Fourth image description.-->
<!--      </h2>-->
<!--    </div>-->
  </div>
</div>
</div>
<!--</section>-->
<!-- End image carousel-->




<!--&lt;!&ndash; Youtube video &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <h2 class="title is-3">Video Presentation</h2>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          -->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; Youtube embed code here &ndash;&gt;-->
<!--            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End youtube video &ndash;&gt;-->


<!--&lt;!&ndash; Video carousel &ndash;&gt;-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title is-3">Another Carousel</h2>-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-video1">-->
<!--          <video poster="" id="video1" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel1.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video2">-->
<!--          <video poster="" id="video2" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel2.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video3">-->
<!--          <video poster="" id="video3" autoplay controls muted loop height="100%">\-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel3.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End video carousel &ndash;&gt;-->






<!-- Paper poster -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->

<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{mirza2023mpvr,
    author    = {Mirza, M. Jehanzeb and Karlinsky, Leonid and Lin, Wei and Doveh, Sivan and
                 and Micorek, Jakub and Kozinski, Mateusz and Kuhene, Hilde and Possegger, Horst},
    journal   = {ArXiv},
    title     = {{Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs}},
    year      = {2024}
    }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            The website template has been shamelesslessly copied from: <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
<!--            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative-->
<!--            Commons Attribution-ShareAlike 4.0 International License</a>.-->
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
